{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import ast\r\n",
    "import datetime\r\n",
    "import os\r\n",
    "import random as python_random\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "from imblearn.under_sampling import RandomUnderSampler\r\n",
    "from keras import Input\r\n",
    "from keras import backend as K\r\n",
    "from keras import optimizers\r\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\r\n",
    "from keras.layers import (Conv2D, Dense, Dropout, Flatten,\r\n",
    "                          MaxPooling2D)\r\n",
    "from keras.models import Sequential\r\n",
    "from keras_preprocessing.image import ImageDataGenerator\r\n",
    "from sklearn.metrics import (accuracy_score, classification_report, f1_score,\r\n",
    "                             multilabel_confusion_matrix)\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\r\n",
    "from talos.utils import early_stopper\r\n",
    "from talos.utils.best_model import activate_model\r\n",
    "from tensorflow.keras.utils import plot_model\r\n",
    "\r\n",
    "import talos\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "np.random.seed(8)\r\n",
    "python_random.seed(8)\r\n",
    "tf.random.set_seed(8)\r\n",
    "\r\n",
    "tf.compat.v1.disable_eager_execution() # May help speed because we are using generators with Talos\r\n",
    "\r\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_label_pivot(ds):\n",
    "    ds.labels = ds.labels.astype(str)\n",
    "    ds_balance = ds[['labels', 'order']]\n",
    "    ds_balance = ds_balance.pivot_table(index=\"order\", columns=\"labels\", aggfunc=len, fill_value=0)\n",
    "    ds_balance.loc[\"Total\"] = ds_balance.sum()\n",
    "    return ds_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, run):\n",
    "    plot_path = Path(\"plots/\" + str(run) + \"_\"+ datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \".pdf\")\n",
    "    plt.plot(history.history['loss'], label='Train MAE')\n",
    "    plt.plot(history.history['val_loss'], label='Val MAE')\n",
    "    plt.plot(history.history['f1score'], label='Train f1')\n",
    "    plt.plot(history.history['val_f1score'], label='Val f1')\n",
    "    plt.plot(history.history['acc'], label='Train Acc')\n",
    "    plt.plot(history.history['val_acc'], label='Val Acc')\n",
    "    plt.title('MAE F1 and Accuracy for CNN')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xlabel('No. epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.savefig(plot_path, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generators(train, val, test, ds_dir, hp):\n",
    "    def orthogonal_rot(image):\n",
    "        return np.rot90(image, np.random.choice([-1, 0, 1]))\n",
    "\n",
    "    if hp['rotation']: preprocessing_function = orthogonal_rot\n",
    "    else: preprocessing_function = None\n",
    "    generator = ImageDataGenerator(\n",
    "        rescale=1./255.,\n",
    "        preprocessing_function=preprocessing_function,\n",
    "        width_shift_range=None,#hp['width_shift'],\n",
    "        height_shift_range=None#hp['height_shift']\n",
    "        )\n",
    "    test_generator = ImageDataGenerator(rescale=1./255.) \n",
    "\n",
    "    train_gen = generator.flow_from_dataframe(\n",
    "        train,\n",
    "        directory=ds_dir,\n",
    "        x_col='filename',\n",
    "        y_col='labels',\n",
    "        class_mode='categorical',\n",
    "        target_size=hp['input_shape'][:2],\n",
    "        batch_size=hp['batch_size'],\n",
    "        color_mode=hp['color_mode']\n",
    "    )\n",
    "    val_gen = test_generator.flow_from_dataframe(\n",
    "        val,\n",
    "        directory=ds_dir,\n",
    "        x_col='filename',\n",
    "        y_col='labels',\n",
    "        class_mode='categorical',\n",
    "        target_size=hp['input_shape'][:2],\n",
    "        batch_size=hp['batch_size'],\n",
    "        color_mode=hp['color_mode']\n",
    "    )\n",
    "    test_gen = test_generator.flow_from_dataframe(\n",
    "        test,\n",
    "        directory=ds_dir,\n",
    "        x_col='filename',\n",
    "        y_col='labels',\n",
    "        class_mode='categorical',\n",
    "        target_size=hp['input_shape'][:2],\n",
    "        batch_size=hp['batch_size'],\n",
    "        color_mode=hp['color_mode'],\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_gen, val_gen, test_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and dataset creation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(DF_PATH, hp, undersample):\n",
    "    ds = pd.read_csv(DF_PATH, usecols=['filename', 'labels', 'order'])\n",
    "    ds = ds.dropna(subset=['labels'])\n",
    "    ds.labels = ds.labels.apply(ast.literal_eval)\n",
    "    print(f\"Original: {ds.shape}\")\n",
    "\n",
    "    if undersample:\n",
    "        rus = RandomUnderSampler(sampling_strategy='all')\n",
    "        ds, _ = rus.fit_resample(ds, ds.labels.astype(str)) \n",
    "    ds = ds.sample(hp[\"samples\"], random_state=8)\n",
    "    # ds_balance = order_label_pivot(ds)\n",
    "    # print(ds_balance)\n",
    "    TRAIN, val_test = train_test_split(ds, test_size=0.2, random_state=8)\n",
    "    VAL, TEST = train_test_split(val_test, test_size=0.5, random_state=8)\n",
    "    print(f\"Train: {TRAIN.shape}\\nVal: {VAL.shape}\\nTest: {TEST.shape}\")\n",
    "    return TRAIN, VAL, TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_model(hp, output_shape=2):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(hp['input_shape'])))\n",
    "    for i in range(1, hp['hidden_layers']+1):\n",
    "        for i in range(1, hp['conv_layers']+1):\n",
    "            model.add(Conv2D(i*hp['first_layer'], (3, 3), activation='relu', kernel_initializer=hp['kernel_initializer'], padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Dropout(hp['dropout']))\n",
    "    model.add(Flatten())\n",
    "    for i in range(1, hp['dense_layers']+1):\n",
    "        model.add(Dense(hp['dense'], activation='relu', kernel_initializer=hp['kernel_initializer']))\n",
    "    model.add(Dense(output_shape, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=hp['lr']), loss='binary_crossentropy', metrics=[\"acc\", talos.utils.metrics.f1score])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper(x, y, x_val, y_val, hp):\n",
    "    DS_DIR = Path('c:/Users/flori/download/subset')\n",
    "    DF_PATH = Path('C:/Users/flori/OneDrive/Documents/Uni/8_Master_thesis/code/subset_logs/20210518-001138.csv')\n",
    "\n",
    "    model = input_model(hp)\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file= 'plots/model_plot_{}.pdf'.format(hp['experiment']), show_shapes=True, show_layer_names=False)\n",
    "    train_df, val_df, test_df = gen_dataset(DF_PATH, hp, undersample=False)\n",
    "    train_gen, val_gen, test_gen = make_generators(train_df, val_df, test_df, DS_DIR, hp)\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=2, verbose=1, mode='max')\n",
    "    mc = ModelCheckpoint('model_tuned_2_224902.h5', monitor='val_acc', mode='max', verbose=1)\n",
    "\n",
    "    out = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=hp['epochs'],\n",
    "        verbose=1\n",
    "        ,callbacks=[es, mc] # \n",
    "        ,workers=8\n",
    "        ,max_queue_size=16\n",
    "    )\n",
    "\n",
    "    plot_history(out, hp['experiment'])\n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples =  281128\r\n",
    "epochs = 10\r\n",
    "\r\n",
    "params = {\r\n",
    "    'experiment': ['final'],\r\n",
    "    \"samples\": [samples],\r\n",
    "    \"epochs\": [epochs],\r\n",
    "\t\"batch_size\": [32],                         # [16, 32, 64]\r\n",
    "    \"rotation\": [True],                         # [True, False]\r\n",
    "    \"width_shift\": [0],                         # [0, 0.1, 0.2],\r\n",
    "    \"height_shift\": [0],                        # [0, 0.1, 0.2],\r\n",
    "    \"input_shape\": [(128,128,1)],               # [(64,64,1), (128,128,1)]\r\n",
    "    \"color_mode\": [\"grayscale\"],                # [\"rgb\", \"grayscale\"]\r\n",
    "\r\n",
    " \t\"lr\": [0.001],                              # [0.0001, 0.001, 0.01]\r\n",
    "    \"hidden_layers\": [3],                       # [2, 3, 4]\r\n",
    "    \"kernel_initializer\": ['glorot_uniform'],   # [he_uniform, glorot_uniform, he_normal]\r\n",
    "    \"conv_layers\": [2],                         # [1,2]\r\n",
    "    \"dropout\": [.3],                            # [.2, .3, .4]\r\n",
    "    \"dense_layers\": [1],                        # [1,2]\r\n",
    "    \"dense\": [1024],                            # [256, 512, 1024, 2048]\r\n",
    "    \"first_layer\": [32],                        # [16, 32, 48, 64]\r\n",
    "}\r\n",
    "\r\n",
    "params_baseline = {\r\n",
    "    'experiment': ['baseline'],\r\n",
    "    \"samples\": [samples],\r\n",
    "    \"epochs\": [epochs],\r\n",
    "\t\"batch_size\": [32],\r\n",
    "    \"rotation\": [True],\r\n",
    "    \"width_shift\": [0.2],\r\n",
    "    \"height_shift\": [0.2],\r\n",
    "    \"input_shape\": [(64,64,1)],\r\n",
    "    \"color_mode\": [\"grayscale\"],\r\n",
    "\r\n",
    " \t\"lr\": [0.001],\r\n",
    "    \"hidden_layers\": [3],\r\n",
    "    \"kernel_initializer\": ['he_uniform'],\r\n",
    "    \"conv_layers\": [1],\r\n",
    "    \"dropout\": [0.3],\r\n",
    "    \"dense_layers\": [3],\r\n",
    "    \"dense\": [1024],\r\n",
    "    \"first_layer\": [64],\r\n",
    "}\r\n",
    "\r\n",
    "dummy_x, dummy_y = np.empty(1), np.empty(1)\r\n",
    "# https://autonomio.github.io/talos/#/README?id=quick-start\r\n",
    "scan = talos.Scan(\r\n",
    "    x=dummy_x,\r\n",
    "    y=dummy_y,\r\n",
    "    model=wrapper,\r\n",
    "    params=params,\r\n",
    "    experiment_name='talos/final',\r\n",
    "    print_params=True,\r\n",
    "    save_weights=True,\r\n",
    "    clear_session=True\r\n",
    "    # ,fraction_limit=.5\r\n",
    "    )\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = scan.best_model(metric='val_f1score', asc=False)\r\n",
    "model.save('models/model_tuned.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = tf.keras.models.load_model(\r\n",
    "    'model_tuned_2_224902.h5',\r\n",
    "    custom_objects={'f1score': talos.utils.metrics.f1score})\r\n",
    "\r\n",
    "DS_DIR = Path('c:/Users/flori/download/subset')\r\n",
    "DF_PATH = Path('C:/Users/flori/OneDrive/Documents/Uni/8_Master_thesis/code/subset_logs/20210518-001138.csv')\r\n",
    "samples =  281128\r\n",
    "hp = {\r\n",
    "    \"samples\": samples,\r\n",
    "\t\"batch_size\": 32,                         # [16, 32, 64]\r\n",
    "    \"rotation\": True,                         # [True, False]\r\n",
    "    \"width_shift\": 0,                         # [0, 0.1, 0.2],\r\n",
    "    \"height_shift\": 0,                        # [0, 0.1, 0.2],\r\n",
    "    \"input_shape\": (128,128,1),                 # [(64,64,1), (128,128,1)]\r\n",
    "    \"color_mode\": \"grayscale\",                # [\"rgb\", \"grayscale\"]\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = gen_dataset(DF_PATH, hp, undersample=False)\r\n",
    "train_gen, val_gen, test_gen = make_generators(train_df, val_df, test_df, DS_DIR, hp)\r\n",
    "model_predict = activate_model(scan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_predict.predict(\r\n",
    "    test_gen,\r\n",
    "    steps=20,\r\n",
    "    verbose=1,\r\n",
    "    workers=8,\r\n",
    "    max_queue_size=8)\r\n",
    "y_pred = np.rint(preds).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.rint(preds).astype(int)\r\n",
    "y_true = np.array(test_gen.classes)\r\n",
    "y_true = MultiLabelBinarizer().fit_transform(y_true)\r\n",
    "print(classification_report(y_true[:len(y_pred)], y_pred))\r\n",
    "print(multilabel_confusion_matrix(y_true[:len(y_pred)], y_pred))\r\n",
    "print(accuracy_score(y_true[:len(y_pred)], y_pred))\r\n",
    "print(f1_score(y_true[:len(y_pred)], y_pred, average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = test_gen.filenames[:len(y_pred)]\r\n",
    "pred_df = pd.DataFrame({'filenames': filenames})\r\n",
    "pred_df['y_pred'] = pd.Series(list(y_pred))\r\n",
    "pred_df['y_true'] = pd.Series(list(y_true))\r\n",
    "pred_df['correct'] = pred_df['y_pred'].astype(str) == pred_df['y_true'].astype(str)\r\n",
    "wrong_preds = pred_df[pred_df.correct == False]\r\n",
    "# wrong_preds.to_csv('models/wrong_preds_test_25000.csv')\r\n",
    "wrong_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the rest of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = tf.keras.models.load_model(\n",
    "    'model_tuned_2_224902.h5',\n",
    "    custom_objects={'f1score': talos.utils.metrics.f1score})\n",
    "\n",
    "DS_DIR = Path('c:/Users/flori/download/original')\n",
    "DF_PATH = Path('C:/Users/flori/OneDrive/Documents/Uni/8_Master_thesis/code/subset_logs/20210518-001138.csv')\n",
    "samples =  281128\n",
    "hp = {\n",
    "    \"samples\": samples,\n",
    "\t\"batch_size\": 32,\n",
    "    \"rotation\": True,\n",
    "    \"width_shift\": 0,\n",
    "    \"height_shift\": 0,\n",
    "    \"input_shape\": (128,128,1),\n",
    "    \"color_mode\": \"grayscale\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(213128,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(213128, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DS_DIR = Path('c:/Users/flori/download/original')\n",
    "orignal_df = pd.DataFrame(os.listdir(Path('c:/Users/flori/download/original')), columns=['filename'])\n",
    "sorted_dir = pd.Series(list(os.walk(DS_DIR))[0][2], name='filename')\n",
    "print(sorted_dir.shape)\n",
    "ds_sorted = pd.merge(sorted_dir, orignal_df, left_on='filename', right_on='filename')\n",
    "ds_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot identify image file 'C:\\\\Users\\\\flori\\\\download\\\\original\\\\desktop.ini'\n",
      "True     213128\n",
      "False         1\n",
      "Name: check_image, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "DS_DIR = Path('c:/Users/flori/download/original')\n",
    "import PIL\n",
    "def check_image(path):\n",
    "    try:\n",
    "        img = PIL.Image.open(path)\n",
    "        img.verify()\n",
    "        img.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "orignal_df['check_image'] = orignal_df.filename.apply(lambda x: check_image(DS_DIR / x))\n",
    "print(orignal_df.check_image.value_counts())\n",
    "orignal_df_checked = orignal_df[orignal_df.check_image == True].drop('check_image', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 213128 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "generator = ImageDataGenerator(rescale=1./255.) \n",
    "original_gen = generator.flow_from_dataframe(\n",
    "    ds_sorted,\n",
    "    directory=DS_DIR,\n",
    "    class_mode='input',\n",
    "    target_size=hp['input_shape'][:2],\n",
    "    batch_size=hp['batch_size'],\n",
    "    color_mode=hp['color_mode'],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(213128, 2)\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "preds = model_predict.predict(\n",
    "    original_gen,\n",
    "    steps=20,\n",
    "    verbose=1,\n",
    "    workers=8,\n",
    "    max_queue_size=8)\n",
    "y_pred = np.rint(preds).astype(int)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('predictions_original.csv', y_pred, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(213128,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DS_DIR = Path('c:/Users/flori/download/original')\n",
    "orignal_df = pd.DataFrame(os.listdir(Path('c:/Users/flori/download/original')), columns=['filename'])\n",
    "sorted_dir = pd.Series(list(os.walk(DS_DIR))[0][2], name='filename')\n",
    "print(sorted_dir.shape)\n",
    "ds_sorted = pd.merge(sorted_dir, orignal_df, left_on='filename', right_on='filename')\n",
    "ds_sorted.shape\n",
    "y_pred = np.genfromtxt('predictions_original.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sorted['pred'] = list(y_pred)\r\n",
    "pred_df = pd.DataFrame(y_pred, columns=['filename', 'prediction'])\r\n",
    "print(ds_sorted.pred.astype(str).value_counts())\r\n",
    "labels = ds_sorted[ds_sorted.pred == list([])]\r\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86a08859f46087af4c4e074af73530229dd7d8037ffe099cb063a6850e6813df"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('python37': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "86a08859f46087af4c4e074af73530229dd7d8037ffe099cb063a6850e6813df"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}